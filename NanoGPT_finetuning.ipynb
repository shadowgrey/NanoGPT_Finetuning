{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGxapT2cdu09"
      },
      "source": [
        "# Introduction\n",
        "we\u2019ll explore how to train a lightweight NanoGPT on the Tiny Stories dataset. NanoGPT, developed by Andrej Karpathy, is a simplified variant of GPT designed for simplicity and speed. Our goal is to generate creative and coherent text based on the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rh4DSQOdu0-"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:36:54.88378Z",
          "iopub.status.busy": "2024-06-20T15:36:54.883342Z",
          "iopub.status.idle": "2024-06-20T15:37:07.278493Z",
          "shell.execute_reply": "2024-06-20T15:37:07.277342Z",
          "shell.execute_reply.started": "2024-06-20T15:36:54.883753Z"
        },
        "id": "VPr4tIjXdu0-",
        "outputId": "d43e0993-daeb-4add-867d-1803ff310942",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.03 KiB | 18.71 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Clone the NanoGPT\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "!pip install tiktoken\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-20T15:35:48.582884Z",
          "iopub.status.busy": "2024-06-20T15:35:48.582534Z",
          "iopub.status.idle": "2024-06-20T15:35:51.155347Z",
          "shell.execute_reply": "2024-06-20T15:35:51.153808Z",
          "shell.execute_reply.started": "2024-06-20T15:35:48.582855Z"
        },
        "id": "KsKY7v1kdu0_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import nanoGPT.model as GPT\n",
        "import wandb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-19T23:15:22.99086Z",
          "iopub.status.busy": "2024-06-19T23:15:22.989777Z",
          "iopub.status.idle": "2024-06-19T23:15:23.083459Z",
          "shell.execute_reply": "2024-06-19T23:15:23.082657Z",
          "shell.execute_reply.started": "2024-06-19T23:15:22.990822Z"
        },
        "id": "GjrpvoD4du0_",
        "outputId": "3463c258-98ff-4919-a608-cae0aa241c30",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33-8\u001b[0m (\u001b[33-8-massachusetts-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Directly set your API key (not secure in public notebooks)\n",
        "import wandb\n",
        "wandb.login(key=\"USE YOUR OWN kEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-20T15:09:34.707716Z",
          "iopub.status.busy": "2024-06-20T15:09:34.707359Z",
          "iopub.status.idle": "2024-06-20T15:09:34.713942Z",
          "shell.execute_reply": "2024-06-20T15:09:34.712624Z",
          "shell.execute_reply.started": "2024-06-20T15:09:34.707684Z"
        },
        "id": "qjqvixGUdu0_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class GPTConfig: # Model config from NanoGPT\n",
        "    block_size: int = 384\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 8\n",
        "    n_head: int = 8\n",
        "    n_embd: int = 384\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "config = GPTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:09:40.77845Z",
          "iopub.status.busy": "2024-06-20T15:09:40.778103Z",
          "iopub.status.idle": "2024-06-20T15:09:40.786668Z",
          "shell.execute_reply": "2024-06-20T15:09:40.785538Z",
          "shell.execute_reply.started": "2024-06-20T15:09:40.778427Z"
        },
        "id": "dfcmOzQ9du0_",
        "outputId": "44594ba3-56d9-40c0-a1f1-6b7d70596a4f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'block_size': 384,\n",
              " 'vocab_size': 50304,\n",
              " 'n_layer': 8,\n",
              " 'n_head': 8,\n",
              " 'n_embd': 384,\n",
              " 'dropout': 0.0,\n",
              " 'bias': True}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb_config = {k:v for k,v in vars(config).items() if not callable(getattr(config, k)) and not k.startswith(\"__\")} # Creating Wandb hyperparameters config for tracking experiements\n",
        "wandb_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "execution": {
          "iopub.execute_input": "2024-06-19T23:15:27.491113Z",
          "iopub.status.busy": "2024-06-19T23:15:27.490277Z",
          "iopub.status.idle": "2024-06-19T23:15:44.976202Z",
          "shell.execute_reply": "2024-06-19T23:15:44.974881Z",
          "shell.execute_reply.started": "2024-06-19T23:15:27.491079Z"
        },
        "id": "FsVaxNjCdu0_",
        "outputId": "66f4d760-714a-4320-d50f-6992f93df815",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_130522-hlvxtchx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories/runs/hlvxtchx' target=\"_blank\">nanoGPT</a></strong> to <a href='https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories' target=\"_blank\">https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories/runs/hlvxtchx' target=\"_blank\">https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories/runs/hlvxtchx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories/runs/hlvxtchx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7d72498c8650>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"nanogpt-tinystories\", name=\"nanoGPT\", config=wandb_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_RfdgYTdu0_"
      },
      "source": [
        "# Tiny Stories datasets and preprocessing\n",
        "\n",
        "TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e3f78e853c5d4fccaed573f05082ff55",
            "116af1d5f4b84e969d77d72843f034c0",
            "18b605242d4c4eb195fe834af7e24660",
            "148dd7a2d92d4bb08bce113b3f534bb6",
            "1641d53d84de40819ea8a899eaedc3ea",
            "fd924f38b2e84c65b701605981f3febd",
            "8ca8c77a76b745779b0511ab66e526f0",
            "f4bfa8df6c67462281de28f6c78a5b4b",
            "80a710a67d6844048f99f8380c1b2d1d",
            "94b8083b60464b17b6869adea74730e5",
            "6fb4b0e9da3546b28b94524426b36da8",
            "8f51cffe3a9449818d99aeffcd76f2d7",
            "cb98ded511ec4f418c263af2fdb7f645",
            "e1001e29b6964f39967b89b4354228c9",
            "f7a7fecf7a70474cbe7bfce1745dcd31",
            "a6c18e19e3e142199dbd506edee475ec",
            "9a1e82c95c9b483bbf3de213837cff80",
            "1b670c8bda8946cb86b8ed24f647f752",
            "c9a8a8b24179411288373106774e0ee8",
            "2a5122c7c8754d5ea931b1c94d594783",
            "42c7857241974029a892ad89dff0a4d5",
            "293b4e835165406184816cfe07f86ca9",
            "f9df1b7c6b40499ab13d8b5e7f7ef584",
            "5e18893e33824c9487f75aba550fe68c",
            "f8c8ddf77a084cd9a8446437d508fc25",
            "32fa532ab28d45f59951733fb3c32115",
            "aa29f78501ef427691402071c5f7c27f",
            "455a96b531dd487dbf32a69e0c1e3db2",
            "865a38a219844cbbb717109e20932334",
            "e564b95d7d044728b565efc0b21a0e09",
            "7f1eb21ca2fc4a73b9336af92fa8fd37",
            "802276bca7c64287a3c5c33ef75c3c8e",
            "675e14d71749407fa4311d3ee0292a4a",
            "9cb185a37b054108b44c46edf5c2ac56",
            "e16dc7c5ceda4069ba3b0d5fefb1f223",
            "7a3def59fe85451b8a7193177519e669",
            "5a180ab1ec774d719b3f998bb0f63abe",
            "9b4802f979ee4b64925f4e4a392fa89b",
            "d703b022d88c4105af9bf829e77ddc0c",
            "4f80d681ea914ad18b4280d6e5075279",
            "cb9f1835c14947429a613e29a9f790d4",
            "07991fffcc72425298e1504c7fde02ba",
            "04997f61fee84189a0347e3b9ea7c55b",
            "1ca54954887a473a8521ad56e820f467",
            "fcbd9d5b954443ceacc10b5a2998c9d2",
            "48ed4a4cfde84451a8f682efb19631e4",
            "3770f909def044e484fe955235fe1dfe",
            "5abaed5d34c5436f9ffc247ac225210b",
            "2660df36c9de435b9003173aac31eb01",
            "5dbcfebf7ffe4ca68b5cf032d1518968",
            "9bc613fcefea40c19e31ad58e27e162b",
            "e4992e8793ab409ca8bafbb18581eba0",
            "fe64069b5a694d5385b8820fe144b2db",
            "1fb1c5496d1f4cc996b82cc7e63a94e8",
            "f7c935a4007e4d92838461d263fa463c",
            "ca2cfee18bb646ff8797e3fac616c057",
            "3c32abc010074e51b4ea433afa7b4bc7",
            "e461b2dd5cf247029231bbff52e574b7",
            "a4bd52ed024144ad83f1cdf6f8c0d7f6",
            "34387505b47543ed84e9c50f768be244",
            "6e81cf5d1a4c4d1eaff356a3f0e8683b",
            "996caa9f8e43490cb8aca6fc4a653f16",
            "08963eb76bc645258bc3e02358a16c53",
            "291604f82f94462483e60513e30bd65c",
            "259246e25fc7446f8c59a972e442d932",
            "b08f73d16702443091746d7e27d637fd",
            "b79f51595ba844699031a12d821b3882",
            "2ffe688d6e6143dfad6ecaf18050027d",
            "cfd195b6320745eebcef34726c8d0100",
            "730080a1b91d4312bd1a59461e6880ac",
            "f57b5fb371e94a94adb8ea75eda5e4f7",
            "fd4002f2d109456b83777c8bbb03513c",
            "e0fde917cee949bb9315a53e84446a1c",
            "a673a1d9364040bdb64d8eb2905141a3",
            "33390669c26d442593c53b97a65478d2",
            "7e09a508025e45ef912ac70f9359bb21",
            "9bda0caacf194d928ce4a39b42c2ec70",
            "bb9b463fb4f6459f88bacc61376a584f",
            "4a2abecdcd4c4652b8590d934c7b438c",
            "f40bab7460be4e049dce529d9a23ca51",
            "37542faf419b42ec9125a2a6a95ff5c1",
            "e6f557d07d554c089aec228941af81c5",
            "bfe89552e9b6464fb39e69cdbed863bd",
            "cc8abec8ed5b4abbaf8f2fa0e4811bac",
            "0d2ed2a55cfa4e2780ef2c1698f1f3bd",
            "2c036e0772bc4ce4b65d80dbe7e30609",
            "06cbf55284b24425acf8a36171cf44e1",
            "e982040f2d8344fa9c7a845bcad405a2"
          ]
        },
        "id": "sDTlxOOSdu0_",
        "outputId": "1987d669-ea51-45a8-cc9e-711b22e60aba",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3f78e853c5d4fccaed573f05082ff55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f51cffe3a9449818d99aeffcd76f2d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(\u2026)-00000-of-00004-2d5a1467fff1081b.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9df1b7c6b40499ab13d8b5e7f7ef584",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(\u2026)-00001-of-00004-5852b56a2bd28fd9.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cb185a37b054108b44c46edf5c2ac56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(\u2026)-00002-of-00004-a26307300439e943.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcbd9d5b954443ceacc10b5a2998c9d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(\u2026)-00003-of-00004-d243063613e5a057.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca2cfee18bb646ff8797e3fac616c057",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(\u2026)-00000-of-00001-869c898b519ad725.parquet:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b79f51595ba844699031a12d821b3882",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9b463fb4f6459f88bacc61376a584f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "ab399ea923d94208ae6510229cbfadbc",
            "401641dd2d5e44d8a0e8a6c43fab159b",
            "67e65296e50842f69f8dca851b5733c5",
            "afcbbf7f4f7746389f0c25392dc04390",
            "5cb12446fd34429680b96bedfe41d4e7",
            "a466982c1ec54f6f8dbb5089c5417e5b",
            "c25b732e5180442dadb25d3ef79b29ed",
            "36aa8e2641c64c89b2fb2e2b557a5c68",
            "3f8299396daa4f1491d9c729a6f39707",
            "ea5c824fdafb4868840005d6ac5bc0a9",
            "7fd31999d9924892aa6f67b8365c1dfd",
            "4305278639b84621bdb59afb9cdebc62",
            "590ef77dfeee4952b7d68b4ed88ced0b",
            "7d4b5b0340fd4891a2e68256afbbd46d",
            "499397eefe334af99840c6a7e78af7e8",
            "d21da38239404385825d84506957291a",
            "e8d1a404597b435a97cf884c4ef71edc",
            "8c7f82d15eda4b28a771aba6d604e49c",
            "7e7c047142d84f87a4492e0774ef2bbf",
            "a2821237f8c5468ea455cd686d6320f1",
            "93a82e83ebc046b5b7f35b97749c1a20",
            "e99ac4f4d3f445fea5d79cd1f67ac071",
            "efb986a96c884f31b397e9362169a5b4",
            "474734ff90fa4677b4ade2f46a69a644",
            "fff46afeb8cb49d99c1ab7f55c80ad88",
            "0dcb9e7d51ce4b9d8beefea682167707",
            "c438dbc0b6c242c99a6b7d4b156bcb9e",
            "ed947a124f41493c95a55f9983e50101",
            "9ef39a0a650d4415b78a52eda2bd182b",
            "7d330f7e1e0f4840a2415ed7ea5d9d63",
            "62fc8cbc6b4d4f1484ea3024f7acf037",
            "70f2174295ed4be69baafd8c0412af59",
            "4f080ac093584b078de72b0102478022",
            "bc949127cd4b4ca9852ea24eb743e889",
            "0bd12c2d3df74641b18e5614263461bf",
            "2ccdbd4281434772960fa7666a0e4102",
            "abff350a07754311b6b9723a8a2fc056",
            "bcac11141d6a44cfa041f75c3da1f95b",
            "0b4dacde0f934f738991c2e054a2ff53",
            "868a943656914ff7a0b59049899be6ef",
            "f77c2aa2a481426fa09c6bb30f25406e",
            "39e7cef12bd949e080dbc81abe289f61",
            "ba6b91ed56f540c6b748b289e74e6bac",
            "03551a0f75684f4db83ac85db2b00554"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:38:31.416375Z",
          "iopub.status.busy": "2024-06-20T15:38:31.416008Z",
          "iopub.status.idle": "2024-06-20T15:38:35.033742Z",
          "shell.execute_reply": "2024-06-20T15:38:35.032488Z",
          "shell.execute_reply.started": "2024-06-20T15:38:31.416342Z"
        },
        "id": "b1X3ygXxdu0_",
        "outputId": "b8ce051d-8254-41fe-fc8c-cc6516ff93a5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab399ea923d94208ae6510229cbfadbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizing the splits (num_proc=8):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4305278639b84621bdb59afb9cdebc62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizing the splits (num_proc=8):   0%|          | 0/21990 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efb986a96c884f31b397e9362169a5b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "writing train.bin:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc949127cd4b4ca9852ea24eb743e889",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "writing validation.bin:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tiktoken\n",
        "import os\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
        "\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
        "    out = {'ids': ids, 'len': len(ids)}\n",
        "    return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = ds.map(\n",
        "        process,\n",
        "        remove_columns=['text'],\n",
        "        desc=\"tokenizing the splits\",\n",
        "        num_proc=8,\n",
        "        )\n",
        "    # concatenate all the ids in each dataset into one large file we can use for training\n",
        "    for split, dset in tokenized.items():\n",
        "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "        filename = f'{split}.bin'\n",
        "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "        total_batches = 1024\n",
        "\n",
        "        idx = 0\n",
        "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "            # Batch together samples for faster write\n",
        "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "            arr_batch = np.concatenate(batch['ids'])\n",
        "            # Write into mmap\n",
        "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "            idx += len(arr_batch)\n",
        "        arr.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2oP2O-rdu0_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LuOfyrPMdu0_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd33vK5Kdu0_",
        "outputId": "d82f0c5e-9c0c-41a9-8ce0-9a529bf601e1",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d7338457950>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training Config\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 4000\n",
        "warmup_steps = 100\n",
        "min_lr = 5e-5\n",
        "eval_iters = 100\n",
        "batch_size = 6\n",
        "block_size = 384\n",
        "\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TyhxHB3du1A",
        "outputId": "39950dfd-e3da-432c-b996-119e10403075",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 33.51M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-d2e02fa34ae2>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "nanoGPT = GPT.GPT(config)\n",
        "optimizer =  torch.optim.AdamW(nanoGPT.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f3a3eaa3a0864daea3c08dc2ec039794",
            "5bfe0b628407461288a0d0b5dd33367d",
            "8a00cbddfee9474392f740413229603f",
            "e2c15056b22b4fb08f1e4066e5952a4b",
            "3726c77233d74323805da18c3f7dacfe",
            "a8e45c1444e240669855be72c8a3814f",
            "6a5e3fbb262b4658be07b0dfe108b212",
            "b227fc4d26a64e508c72c56f9318f551",
            "0bf2c7e0f8e0429e8e624c82a1fd367a",
            "80fbe781c56c4b4c8263d09c54ad6936",
            "207ddaded37a4680b124c6f0a379d4b2"
          ]
        },
        "id": "3blYH15ydu1A",
        "outputId": "b314ecc6-aa9d-4d81-a8c3-4b865f8be9f9",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3a3eaa3a0864daea3c08dc2ec039794",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100: train loss 5.7328, val loss 5.7564\n",
            "The current learning rate: 0.0010\n",
            "Epoch 200: train loss 5.2284, val loss 5.2127\n",
            "The current learning rate: 0.0010\n",
            "Epoch 300: train loss 4.6277, val loss 4.6375\n",
            "The current learning rate: 0.0010\n",
            "Epoch 400: train loss 4.2943, val loss 4.2759\n",
            "The current learning rate: 0.0010\n",
            "Epoch 500: train loss 4.0867, val loss 4.1038\n",
            "The current learning rate: 0.0010\n",
            "Epoch 600: train loss 3.9639, val loss 3.9806\n",
            "The current learning rate: 0.0010\n",
            "Epoch 700: train loss 3.8763, val loss 3.8831\n",
            "The current learning rate: 0.0009\n",
            "Epoch 800: train loss 3.7918, val loss 3.7897\n",
            "The current learning rate: 0.0009\n",
            "Epoch 900: train loss 3.6709, val loss 3.6681\n",
            "The current learning rate: 0.0009\n",
            "Epoch 1000: train loss 3.5811, val loss 3.5691\n",
            "The current learning rate: 0.0009\n",
            "Epoch 1100: train loss 3.5323, val loss 3.5084\n",
            "The current learning rate: 0.0009\n",
            "Epoch 1200: train loss 3.3961, val loss 3.4399\n",
            "The current learning rate: 0.0008\n",
            "Epoch 1300: train loss 3.3356, val loss 3.3519\n",
            "The current learning rate: 0.0008\n",
            "Epoch 1400: train loss 3.2753, val loss 3.3133\n",
            "The current learning rate: 0.0008\n",
            "Epoch 1500: train loss 3.1934, val loss 3.2054\n",
            "The current learning rate: 0.0007\n",
            "Epoch 1600: train loss 3.1742, val loss 3.1362\n",
            "The current learning rate: 0.0007\n",
            "Epoch 1700: train loss 3.0776, val loss 3.0970\n",
            "The current learning rate: 0.0007\n",
            "Epoch 1800: train loss 3.0275, val loss 3.0450\n",
            "The current learning rate: 0.0006\n",
            "Epoch 1900: train loss 2.9913, val loss 2.9723\n",
            "The current learning rate: 0.0006\n",
            "Epoch 2000: train loss 2.9500, val loss 2.9166\n",
            "The current learning rate: 0.0005\n",
            "Epoch 2100: train loss 2.8972, val loss 2.9240\n",
            "The current learning rate: 0.0005\n",
            "Epoch 2200: train loss 2.8551, val loss 2.8719\n",
            "The current learning rate: 0.0005\n",
            "Epoch 2300: train loss 2.8277, val loss 2.8518\n",
            "The current learning rate: 0.0004\n",
            "Epoch 2400: train loss 2.7901, val loss 2.8106\n",
            "The current learning rate: 0.0004\n",
            "Epoch 2500: train loss 2.7605, val loss 2.7703\n",
            "The current learning rate: 0.0004\n",
            "Epoch 2600: train loss 2.7464, val loss 2.7550\n",
            "The current learning rate: 0.0003\n",
            "Epoch 2700: train loss 2.7214, val loss 2.7165\n",
            "The current learning rate: 0.0003\n",
            "Epoch 2800: train loss 2.6962, val loss 2.6905\n",
            "The current learning rate: 0.0003\n",
            "Epoch 2900: train loss 2.6720, val loss 2.6689\n",
            "The current learning rate: 0.0002\n",
            "Epoch 3000: train loss 2.6630, val loss 2.6491\n",
            "The current learning rate: 0.0002\n",
            "Epoch 3100: train loss 2.6288, val loss 2.6557\n",
            "The current learning rate: 0.0002\n",
            "Epoch 3200: train loss 2.6169, val loss 2.6440\n",
            "The current learning rate: 0.0001\n",
            "Epoch 3300: train loss 2.6414, val loss 2.6023\n",
            "The current learning rate: 0.0001\n",
            "Epoch 3400: train loss 2.5963, val loss 2.6212\n",
            "The current learning rate: 0.0001\n",
            "Epoch 3500: train loss 2.6056, val loss 2.6356\n",
            "The current learning rate: 0.0001\n",
            "Epoch 3600: train loss 2.5745, val loss 2.5852\n",
            "The current learning rate: 0.0001\n",
            "Epoch 3700: train loss 2.5822, val loss 2.5779\n",
            "The current learning rate: 0.0001\n",
            "Epoch 3800: train loss 2.5410, val loss 2.5740\n",
            "The current learning rate: 0.0001\n",
            "Epoch 3900: train loss 2.5683, val loss 2.5842\n",
            "The current learning rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>\u2581\u2581\u2581\u2582\u2582\u2582\u2582\u2582\u2582\u2583\u2583\u2583\u2583\u2583\u2584\u2584\u2584\u2584\u2584\u2585\u2585\u2585\u2585\u2585\u2585\u2586\u2586\u2586\u2586\u2586\u2587\u2587\u2587\u2587\u2587\u2587\u2588\u2588\u2588</td></tr><tr><td>lr</td><td>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2587\u2587\u2587\u2587\u2587\u2586\u2586\u2586\u2586\u2585\u2585\u2585\u2585\u2584\u2584\u2584\u2584\u2583\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581</td></tr><tr><td>train/loss</td><td>\u2588\u2587\u2586\u2585\u2584\u2584\u2584\u2584\u2583\u2583\u2583\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581</td></tr><tr><td>val/loss</td><td>\u2588\u2587\u2586\u2585\u2584\u2584\u2584\u2584\u2583\u2583\u2583\u2583\u2583\u2583\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3900</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>2.56834</td></tr><tr><td>val/loss</td><td>2.58416</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">nanoGPT</strong> at: <a href='https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories/runs/hlvxtchx' target=\"_blank\">https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories/runs/hlvxtchx</a><br> View project at: <a href='https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories' target=\"_blank\">https://wandb.ai-8-massachusetts-institute-of-technology/nanogpt-tinystories</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_130522-hlvxtchx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch%eval_iters == 0 and epoch != 0:\n",
        "        losses = estimate_loss(nanoGPT)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.4f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        wandb.log({\n",
        "                \"epoch\": epoch,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(nanoGPT.state_dict(), best_model_params_path)\n",
        "\n",
        "    X,y = get_batch(\"train\")\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = nanoGPT(X, y)\n",
        "        loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(nanoGPT.parameters(), 1)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ab-waPo5du1A",
        "outputId": "e874afd4-5513-4755-a3ba-57a8f0240a4d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVn5JREFUeJzt3Xd0VGXixvHvpE16CC0FQmgSauglKCAdRKW4goCiu6gr4iq6uj9ZG+IqrF1XF0VXsFAEVsCG9N5LQid0QkkCAmmQRub9/TFLNAKhmOQmk+dzzj3JzLwz89xcD/N457332owxBhEREREX4WZ1ABEREZGipHIjIiIiLkXlRkRERFyKyo2IiIi4FJUbERERcSkqNyIiIuJSVG5ERETEpXhYHaCkORwOTpw4QUBAADabzeo4IiIicg2MMaSnpxMeHo6bW+H7ZspduTlx4gQRERFWxxAREZEbcPToUapXr17omHJXbgICAgDnHycwMNDiNCIiInIt0tLSiIiIyP8cL0y5KzcXv4oKDAxUuRERESljrmVKiSYUi4iIiEtRuRERERGXonIjIiIiLkXlRkRERFyKyo2IiIi4FJUbERERcSkqNyIiIuJSVG5ERETEpajciIiIiEtRuRERERGXonIjIiIiLkXlRkRERFyKyk1ROnEC9u+3OoWIiEi5pnJTVD74AKpVg2eftTqJiIhIuaZyU1QaN3b+3LjR2hwiIiLlnMpNEVlTJRuHDUhIgORkq+OIiIiUWyo3ReSI4wy7K//vhvbeiIiIWEblpoi0j2jPhmrO33PXrrY2jIiISDmmclNEagTVYF/tIADSVy+1OI2IiEj5pXJTRGw2G3mtWgLgE7sdjLE4kYiISPmkclOEwm7uSbY7+KSdh0OHrI4jIiJSLqncFKG2tToQF+r83axfb20YERGRckrlpgi1CGvBlurOP2nKqkUWpxERESmfVG6KkN3DzulGtQHIWbvK4jQiIiLlk8pNEfOKuQWACrsOwoULFqcREREpf1RuiljdmD6keYE9+wLs2mV1HBERkXJH5aaIxdS4mU3hzt8z16ywNoyIiEg5pHJTxMICwthbOxCA08t/sjiNiIhI+aNyUwyyWkQD4L55i8VJREREyh+Vm2JQoUN3AKoeSILMTIvTiIiIlC8qN8UgulUfkvzA3WFwxGrvjYiISElSuSkG0aFN80/ml7zkO4vTiIiIlC8qN8XAw82DpIY1ADi3Zpm1YURERMoZlZti4ta6LQD+W3dbnERERKR8UbkpJuFd+gIQeiINzp61OI2IiEj5oXJTTFo26cGBYOfvaauWWBtGRESkHFG5KSaVfCuxp1YAACeWfmtxGhERkfLD0nIzZswYbDZbgaV+/fpXHD958uRLxnt7e5dg4uuT1tS5Lo716yxOIiIiUn54WB2gUaNGLFq0KP+2h0fhkQIDA4mPj8+/bbPZii3b7+V3860waSNVdx22OoqIiEi5YXm58fDwIDQ09JrH22y26xpvpbpd7ibP9gaVU3K4cPQIHhGRVkcSERFxeZbPudm3bx/h4eHUrl2boUOHkpCQUOj4jIwMIiMjiYiIoG/fvuzcubPQ8dnZ2aSlpRVYSkr9mi3ZHeL8Ex9ZOKvE3ldERKQ8s7TctG3blsmTJ/PTTz8xYcIEDh06RIcOHUhPT7/s+KioKD777DPmzp3LV199hcPhoH379hw7duyK7zFu3DiCgoLyl4iIiOJanUu42dw4FhUGQMqKBSX2viIiIuWZzRhjrA5xUUpKCpGRkbz99tsMHz78quNzc3Np0KABgwcP5pVXXrnsmOzsbLKzs/Nvp6WlERERQWpqKoGBgUWW/Uq+f/pObn/rO7ZHh9Jka2Kxv5+IiIgrSktLIygo6Jo+vy3/WurXKlSoQL169di/f/81jff09KR58+aFjrfb7QQGBhZYSlLljr0BqLHvJDgcJfreIiIi5VGpKjcZGRkcOHCAsLCwaxqfl5fH9u3br3m8FRp2GUimBwRlOjgZt8bqOCIiIi7P0nLz9NNPs3z5cg4fPsyaNWvo378/7u7uDB48GIBhw4YxevTo/PFjx45lwYIFHDx4kC1btnDvvfdy5MgRHnzwQatW4aoC/SsRX8MXgCOLNKlYRESkuFl6KPixY8cYPHgwp0+fpkqVKtxyyy2sW7eOKlWqAJCQkICb2y/96+zZszz00EMkJSURHBxMy5YtWbNmDQ0bNrRqFa7JmUa14eAOstautDqKiIiIyytVE4pLwvVMSCoqK197hA7Pfcz2OgE02V9yh6KLiIi4ijI7odhVRXQdAEDdI+lkZ2ZYnEZERMS1qdyUgMhWXUnxseFzAeKX/9fqOCIiIi5N5aYE2NzdOVynEgDJS7+3OI2IiIhrU7kpIeebNwbAtnGTxUlERERcm8pNCQm6pRsA4XuOUc7mcIuIiJQolZsSUrvnPQBEJV3g6Ik9FqcRERFxXSo3JcQnsg4nK3jibmDvwulWxxEREXFZKjclKKmB84rk6auXWJxERETEdanclKTWrQHwi9tpcRARERHXpXJTgkK63AlAnQNnOZdzzuI0IiIirknlpgRV7dgbgDpnIW7HIovTiIiIuCaVmxJkCw7mRJg/AMeWzLE2jIiIiItSuSlhqU3qAXBh/VqLk4iIiLgmlZsS5nNzJwAq7zyok/mJiIgUA5WbElatSz8AmiXksu/0XmvDiIiIuCCVmxLm2bI1F9xthJyDbRt/sDqOiIiIy1G5KWk+PiTVqgLA6eXzLA4jIiLielRuLJDbvCkAnlvirA0iIiLiglRuLFCxUy8Aau37mdSsVIvTiIiIuBaVGwsEdegGQMsTsD5hjcVpREREXIvKjRUaNiTL7k5gDuxb873VaURERFyKyo0VPDw4Uz8SgOx1qy0OIyIi4lpUbixi/neF8Irb91ucRERExLWo3FikYoceAEQlnOPUuVMWpxEREXEdKjcW8Wl7MwBNk2Drsc0WpxEREXEdKjdWuekmMr098L0ARzcstDqNiIiIy1C5sYqbG6fqVQcge4MOBxcRESkqKjcWymvhPFOx3454i5OIiIi4DpUbC1WI6QJArYNnOZ973uI0IiIirkHlxkIVbu4KQLNE2H4iztowIiIiLkLlxkK2+vXJ8nLDPxcOb1hgdRwRERGXoHJjJXd3kuqGAXBu/UqLw4iIiLgGlRuL5TRrDIDP1l0WJxEREXENKjcWC2jXCYBq+0+S58izOI2IiEjZp3JjsaodewPQ7ISDvaf2WJxGRESk7FO5sZh7o8Zke9oIzIH9G+dbHUdERKTMU7mxmocHJ2pXASB9zTJrs4iIiLgAlZtS4HyTBgB4bN1mcRIREZGyT+WmFPBpewsAoXtPYIyxOI2IiEjZpnJTCoR37ANAk2O5nEg7bnEaERGRsk3lphTwbtaSHA8bwVkQv0mTikVERH4PS8vNmDFjsNlsBZb69esX+pyZM2dSv359vL29adKkCT/++GMJpS1GXl4ciwwG4OyqRRaHERERKdss33PTqFEjEhMT85dVq1ZdceyaNWsYPHgww4cPJzY2ln79+tGvXz927NhRgomLR3rjmwCwxcZanERERKRss7zceHh4EBoamr9Urlz5imPfe+89evXqxTPPPEODBg145ZVXaNGiBR988EEJJi4eXq3aAVB1T4LFSURERMo2y8vNvn37CA8Pp3bt2gwdOpSEhCt/uK9du5Zu3boVuK9nz56sXbv2is/Jzs4mLS2twFIahd16OwD1EzJJy0q1OI2IiEjZZWm5adu2LZMnT+ann35iwoQJHDp0iA4dOpCenn7Z8UlJSYSEhBS4LyQkhKSkpCu+x7hx4wgKCspfIiIiinQdikqF1h3IdYPKmbBn8wKr44iIiJRZlpab3r17c/fddxMdHU3Pnj358ccfSUlJYcaMGUX2HqNHjyY1NTV/OXr0aJG9dpGy20mICATg1CodMSUiInKjPKwO8GsVKlSgXr167N+//7KPh4aGkpycXOC+5ORkQkNDr/iadrsdu91epDmLS0qj2nAkDsfmjVZHERERKbMsn3PzaxkZGRw4cICwsLDLPh4TE8PixYsL3Ldw4UJiYmJKIl6xc2/ZGoDgXYetDSIiIlKGWVpunn76aZYvX87hw4dZs2YN/fv3x93dncGDBwMwbNgwRo8enT/+iSee4KeffuKtt95iz549jBkzhk2bNvHYY49ZtQpFqmqH3gDcdDiNnAvZFqcREREpmywtN8eOHWPw4MFERUUxcOBAKlWqxLp166hSxXmV7ISEBBITE/PHt2/fnqlTpzJx4kSaNm3KrFmzmDNnDo0bN7ZqFYpU2M09ueAGIedg3/blVscREREpk2ymnF2pMS0tjaCgIFJTUwkMDLQ6ziUORPhT59g5lrzzBF1GvWt1HBERkVLhej6/S9WcG4HT9WsAkLNpncVJREREyiaVm1LG0aIFAIE79lmcREREpGxSuSllKt3sPANzzUNnKWffGIqIiBQJlZtSJvLWvuTZIDzNcDRe57sRERG5Xio3pYxXYDCHQ70BOLZsrsVpREREyh6Vm1IouV41ADI3rLY4iYiISNmjclMKXWgeDYDftj0WJxERESl7VG5KoaCYzgBEHPjZ4iQiIiJlj8pNKVSzc38AqqXkceZIvMVpREREyhaVm1IoqEp1Dlb1BODI0tkWpxERESlbVG5KqeM3hQKQsXaFxUlERETKFpWbUio7uiEA9m07LE4iIiJStqjclFJ+7ToCEL4vyeIkIiIiZYvKTSkVeWs/AKqfziUz+bi1YURERMoQlZtSKiyiAYcqOTdPwtI51oYREREpQ1RuSimbzUZCnSoApKxZYnEaERGRskPlphQ71yQKAI+4rRYnERERKTtUbkoxe9ubAagarzk3IiIi10rlphSL6HiH8+fJLPLOnrE4jYiISNmgclOK1bmpDYcr2AA4vuJ7i9OIiIiUDSo3pZi7mzuHalcA4MyqhdaGERERKSNUbkq51EZ1ATBbtlicREREpGxQuSnlPFu1BaDy7iMWJxERESkbVG5KudCOtwFQLekcpKdbnEZERKT0U7kp5Ro06sTRQHAz8POaxVbHERERKfVUbko5X09f9tb0B+DUinkWpxERESn9VG7KgDMNagFwYfMGi5OIiIiUfio3ZYCtZUsAKuw8aHESERGR0k/lpgyo0qEXAOHH0+DcOYvTiIiIlG4qN2VAwyZdOOEP7gbObVxrdRwREZFSTeWmDKjiV4WdNbwBSF72ncVpRERESjeVmzLiaNOaALjPm29tEBERkVJO5aaMyLvTeYXwapv3QkqKtWFERERKMZWbMqJHr5HsqAIeeYazs6ZYHUdERKTUUrkpIyIrRLKxTTUAfp72qcVpRERESi+VmzLEftdAAKqt3gZZWRanERERKZ1UbsqQTgOe4mgg+GY7ODV3mtVxRERESiWVmzKkWlB1NrQOAyDpq48sTiMiIlI6qdyUMR4D7gag2vItkJdncRoREZHSR+WmjGk3+BnO+EDF9AucmD/L6jgiIiKljspNGRMSXJ3NzUMBOPb5vyxOIyIiUvqo3JRF/fsDEL54IxhjcRgREZHSpdSUm/Hjx2Oz2Rg1atQVx0yePBmbzVZg8fb2LrmQpUSL+58l0wOqn87h8Apda0pEROTXSkW52bhxIx9//DHR0dFXHRsYGEhiYmL+cuTIkRJIWLpUqlKD2KZVATg8+R2L04iIiJQulpebjIwMhg4dyieffEJwcPBVx9tsNkJDQ/OXkJCQQsdnZ2eTlpZWYHEFjv9daypk0TqLk4iIiJQulpebkSNH0qdPH7p163ZN4zMyMoiMjCQiIoK+ffuyc+fOQsePGzeOoKCg/CUiIqIoYlsu+k9/J88GDY5lEb/xJ6vjiIiIlBqWlpvp06ezZcsWxo0bd03jo6Ki+Oyzz5g7dy5fffUVDoeD9u3bc+zYsSs+Z/To0aSmpuYvR48eLar4lgqsXptdDSoBcGDy2xanERERKT08rHrjo0eP8sQTT7Bw4cJrnhQcExNDTExM/u327dvToEEDPv74Y1555ZXLPsdut2O324skc2mTc/ttsOtLKi1YiTEGm81mdSQRERHLWbbnZvPmzZw8eZIWLVrg4eGBh4cHy5cv5/3338fDw4O8azj7rqenJ82bN2f//v0lkLj0ifrT3wBodSCL7TuXWJxGRESkdLCs3HTt2pXt27cTFxeXv7Rq1YqhQ4cSFxeHu7v7VV8jLy+P7du3ExYWVgKJSx//qMYcrFkBdwPxk9+yOo6IiEipYNnXUgEBATRu3LjAfX5+flSqVCn//mHDhlGtWrX8OTljx46lXbt21K1bl5SUFN544w2OHDnCgw8+WOL5S4vMPj3gwxkE/7QM84a+mhIREbH8aKnCJCQkkJiYmH/77NmzPPTQQzRo0IDbbruNtLQ01qxZQ8OGDS1Maa3af/orALfsyWTT3mXWhhERESkFbMaUr/P3p6WlERQURGpqKoGBgVbH+f2MITkskJDkDCY9fzt/fEVnLBYREddzPZ/fpXrPjVwDm4303l0ACJy3GIdxWBxIRETEWio3LqDG/U8A0GVnJmsOLLc4jYiIiLVUblyAV4dOpAV6E5wFsV+/a3UcERERS6ncuAJ3d1J6dgLA78eF5Dmufo4gERERV6Vy4yLC7n0EgB7bM1l+eJm1YURERCykcuMiPHv0Isvbk+rpsHbOB1bHERERsYzKjavw9ia1i/O6Wz7f/0RuXq7FgURERKyhcuNCKg92nqm51/YsFh9abHEaERERa6jcuBD32+8gz92Nhj/D8gUTrY4jIiJiCZUbV1KhAqntWwLg+d08si9kWxxIRESk5KncuJgKg+4HoPeOLBYcWGBxGhERkZKncuNi3Pr3ByDmGPy0apLFaUREREqeyo2rCQ8nvXkjADy++5HM3EyLA4mIiJQslRsX5H/3UAB678hm+o7pFqcREREpWSo3Lsg2YAAAXQ7Bm/Ne0N4bEREpV1RuXFFUFI6oKLwccPvS4/xrw7+sTiQiIlJiVG5clNuzzwLw8lKY+d9XOH3+tMWJRERESobKjau6/35Mjx5458F7MzN4bdkrVicSEREpETdUbo4ePcqxY8fyb2/YsIFRo0YxcaLOiltq2GzYPv2UC36+tD8G/OtfHDp7yOpUIiIixe6Gys2QIUNYunQpAElJSXTv3p0NGzbw3HPPMXbs2CINKL9DRAQeb78DwCuLHLw/5XGLA4mIiBS/Gyo3O3bsoE2bNgDMmDGDxo0bs2bNGqZMmcLkyZOLMp/8Xg89RPotbfC9AP3e/J5NxzZYnUhERKRY3VC5yc3NxW63A7Bo0SLuvPNOAOrXr09iYmLRpZPfz2Yj4IvpZHl70OkIrB59L8YYq1OJiIgUmxsqN40aNeKjjz5i5cqVLFy4kF69egFw4sQJKlWqVKQBpQjUqsX5l18AYPjX+1i2VJdlEBER13VD5eaf//wnH3/8MbfeeiuDBw+madOmAHz77bf5X1dJ6VLx6ec52KQ6/rng/ejj5OVdsDqSiIhIsbCZG/yOIi8vj7S0NIKDg/PvO3z4ML6+vlStWrXIAha1tLQ0goKCSE1NJTAw0Oo4JSp1x2Y8W7TCNxdWPzeMm//xudWRRERErsn1fH7f0J6bzMxMsrOz84vNkSNHePfdd4mPjy/Vxaa8C2rckg0jnPOjot/8kswD8RYnEhERKXo3VG769u3LF198AUBKSgpt27blrbfeol+/fkyYMKFIA0rRinl9Gptr2gnINpwYegdocrGIiLiYGyo3W7ZsoUOHDgDMmjWLkJAQjhw5whdffMH7779fpAGlaNntviS+9w+y3KHO+n2kTfzA6kgiIiJF6obKzfnz5wkICABgwYIFDBgwADc3N9q1a8eRI0eKNKAUvdvueIqP+4YD4PH0M3DihMWJREREis4NlZu6desyZ84cjh49yvz58+nRowcAJ0+eLHeTdMsiN5sb0eMnsyEcfDOyOTf8Pn09JSIiLuOGys2LL77I008/Tc2aNWnTpg0xMTGAcy9O8+bNizSgFI/ON3Xns8dvIdsd/H5aAlOnWh1JRESkSNzwoeBJSUkkJibStGlT3NycHWnDhg0EBgZSv379Ig1ZlMrzoeC/tT15O18PieYfSyC3QiCeu+MhNNTqWCIiIpco9kPBAUJDQ2nevDknTpzIv0J4mzZtSnWxkYKahDQh+dH72RIKnilpmJEj9fWUiIiUeTdUbhwOB2PHjiUoKIjIyEgiIyOpUKECr7zyCg6Ho6gzSjF6qfs/GHGXnVw3sH3zDYwfb3UkERGR3+WGys1zzz3HBx98wPjx44mNjSU2NpbXXnuNf/3rX7zwwgtFnVGKUfXA6nTr/1f+2uN/d/z97zBxoqWZREREfo8bmnMTHh7ORx99lH818Ivmzp3Lo48+yvHjx4ssYFHTnJtLpWalUvdfdXniu595fiUYmw3bjBnwhz9YHU1ERAQogTk3Z86cuezcmvr163PmzJkbeUmxUJB3EFMHTGVsNw8+agk2YzBDhsDChVZHExERuW43VG6aNm3KBx9cembbDz74gOjo6N8dSkpe9zrd+equKTzWB2Y2BFtuLvTvDxs2WB1NRETkunjcyJNef/11+vTpw6JFi/LPcbN27VqOHj3Kjz/+WKQBpeQMbDSQlDtTuNfxZypkQfeD56B3b1i1Cho0sDqeiIjINbmhPTedOnVi79699O/fn5SUFFJSUhgwYAA7d+7kyy+/LOqMUoIebvkwY3q8Rv9BsL4acOYMdO8OuqyGiIiUETd8Er/L2bp1Ky1atCAvL6+oXrLIaULx1RljeGbhM0xe9BYrJkPDU0C9es49OFWqWB1PRETKoRI5iV9RGz9+PDabjVGjRhU6bubMmdSvXx9vb2+aNGmir8GKgc1m443ub3DnzX+kx72QEATs3ev8iio93ep4IiIihSoV5Wbjxo18/PHHV52MvGbNGgYPHszw4cOJjY2lX79+9OvXjx07dpRQ0vLDZrMx8Y6JtG7bj+73wc++Nti8Gfr1g6wsq+OJiIhckeXlJiMjg6FDh/LJJ58QHBxc6Nj33nuPXr168cwzz9CgQQNeeeUVWrRocdkjty7Kzs4mLS2twCLXxsPNg2l3TaNaq870vNeQ4WWDJUtgyBC4cMHqeCIiIpd1XUdLDRgwoNDHU1JSrjvAyJEj6dOnD926deMf//hHoWPXrl3LU089VeC+nj17MmfOnCs+Z9y4cbz88svXnUucvD28mXPPHLpkd+GO7M389BXYZ8+GRx6BTz4Bm83qiCIiIgVcV7kJCgq66uPDhg275tebPn06W7ZsYePGjdc0PikpiZCQkAL3hYSEkJSUdMXnjB49ukAhSktLIyIi4pozCgTaA5k3dB4dcjpwT1Y8s2aA+3/+A8HB8PrrKjgiIlKqXFe5mTRpUpG98dGjR3niiSdYuHAh3t7eRfa6v2W327Hb7cX2+uVFFb8qLLxvITfn3szDmUf5z7fAm2/Czz/Dxx+Dl5fVEUVERAAL59xs3ryZkydP0qJFCzw8PPDw8GD58uW8//77eHh4XPZw8tDQUJKTkwvcl5ycTGhoaEnFLtcigiJYeN9Cvr2lMn++HS64AZMnQ69ecPas1fFEREQAC8tN165d2b59O3FxcflLq1atGDp0KHFxcbi7u1/ynJiYGBYvXlzgvoULF+afJVmKX1TlKH4a+hPT2gdw+2BIt9tg6VJo3x4OHrQ6noiIiHXlJiAggMaNGxdY/Pz8qFSpEo0bNwZg2LBhjB49Ov85TzzxBD/99BNvvfUWe/bsYcyYMWzatInHHnvMqtUol1qGt2TlH1eyv00dbv6j4WggsGcPpm1bWLPG6ngiIlLOWX4oeGESEhJITEzMv92+fXumTp3KxIkTadq0KbNmzWLOnDn5ZUhKTtPQpmx6eBN1bu1H2wdhcxjYfv4Z06ULfP211fFERKQcK9LLL5QFuvxC0TLG8OaaNxk77//4cpahX/z/Hnj1VRg9WkdSiYhIkSiTl1+Qsslms/HMzc/w3YNLePRPVXm73f8eeO45GD4ccnIszSciIuWPyo0UiVtr3srmEXHMebgDI2+DPBswaRKOXr3gBk7uKCIicqNUbqTIhAWEsXjYYnyfeJo7BkO6F7gtXUpuuzZw6JDV8UREpJxQuZEi5enuyRs93mD4c7Po+WdfjgWAZ/w+clq3gHXrrI4nIiLlgMqNFIu7Gt7F5JdiuX90FFtCwet0Chc6dcDx5RdWRxMRERenciPFpl6lenz7181MeOse5kaBR84F3Ibdz/kn/wKXOQO1iIhIUVC5kWLl5+XHxMFTSf5yAv/s5DzrtO+7H3C6RwdIS7M4nYiIuCKVGyl2NpuNh1s/Qu+vt/DXB8LJ9IBKS9ZyKroueXvjr/4CIiIi10HlRkpMdEg0L38czz9fu43jAVDlyCnOtWjMme9nWh1NRERciMqNlCh/L3/GPPMDq//7Lhsi3Ag8d4HAvgOJH/MXKF8nyxYRkWKiciOWGNj9CQJXb+bbNhXwcEDUyx+w8c5WXMg6b3U0EREp41RuxDL1I5rRfeVxZj3QBgfQ+vst7GgWTuLBbVZHExGRMkzlRizl4+XLHyatZ8WHz5Bqh2bxqeS2as6q7/5tdTQRESmjVG6kVLj10dc5u/RHEqrYqXHWQbO7RjJ93FCrY4mISBmkciOlRs2Y3lTdcZA9zSLwz4UBz09lxtvDrY4lIiJljMqNlCreVcOpv/Egu7tE4+WAPs9+xtzP/s/qWCIiUoao3Ejp4+FBg3kbiW9dG79c6PTo68ybNd7qVCIiUkao3Ejp5OVFvaXbONAwlArZ0OKB0SxdMNHqVCIiUgao3EipZfPzo9aqnRyuFUzIOagz8M+sXauzGYuISOFUbqRUcwuuSLVV2zge6keNVKjYdxCx2xZYHUtEREoxlRsp9TzDq1Np9RZOVrQTdcpgu+02dh9Yb3UsEREppVRupEzwrl0Pv6WrOevvQbPjeaT06MihE7usjiUiIqWQyo2UGX7RLXGbv4B0bzdiDuZwqHsrjp8+bHUsEREpZVRupEwJat+ZnNmzyPK00WVXJrG9mvJzxkmrY4mISCmiciNlTqVe/Un96j/kusHtm9JYdntj0rJSrY4lIiKlhMqNlEkhA//IyY/exGGDPyw/xdy7m5CZm2l1LBERKQVUbqTMqvbQXzk2/u8A3Pf9UabcHUVi2gmLU4mIiNVUbqRMq/G3Vzn07CMAPDj3KEt61GXNgeUWpxIRESup3EiZV2vcBE6+/hJ5Nhi6PpP0Xp35ZOlbGGOsjiYiIhZQuRGXUPWZMeTM+posuzs99xtaD36apyYN0jwcEZFySOVGXIbPgIHYV67lXEV/miXDU0/O5IF/tORIyhGro4mISAlSuRGXYmvdGr/N2zlXpwYRafDJ+N089XQTFh1cZHU0EREpISo34npq1sRvYxxZN7cjMAemT0pn2l978Prq1zUPR0SkHFC5EdcUHIz34mXk3TMITwf8Z44h87n/Y+DMu0nPTrc6nYiIFCOVG3FddjvuU6ZiRo8G4KXlcPtr/6XDx23Ze3qvxeFERKS4qNyIa3Nzw/baazBxIsbdnfu3wlvv7qbb+60Yv2o8p8+ftjqhiIgUMZUbKR8eegjbd9/h8Pej6yH4cUI6X04bTfV3qvPQtw+xLXmb1QlFRKSIqNxI+dG7N24rV2HCwmh8CrZNgH/PymL+0k9p+lFTOn/emdm7Z5PnyLM6qYiI/A4qN1K+NGuGbf166N8fdwN/jIP9H7rx9nwb23YtY8CMAdR5vw5vrH6DM5lnrE4rIiI3wGbK2bGxaWlpBAUFkZqaSmBgoNVxxErr18Ozz8KyZQBk+dp5q4Mbr7XM5LwX+Hj4MKzpMP7S5i80qtrI2qwiIuXc9Xx+q9xI+WYMzJ/vLDlbtwJwvlIg73YP4KWbjnPB3Tmse+3ufNb3M6oHVrcwrIhI+XU9n9+Wfi01YcIEoqOjCQwMJDAwkJiYGObNm3fF8ZMnT8ZmsxVYvL29SzCxuBybDXr1gi1bYMoUqFUL39Np/H36cdK+qMbbZ9rgbmwsPLiQO6bdwbmcc1YnFhGRq7C03FSvXp3x48ezefNmNm3aRJcuXejbty87d+684nMCAwNJTEzMX44c0XWDpAi4ucGQIbBnD/zrX1C1Kj5HjvPk+xvImN2Qu48FEZcUx32z78NhHFanFRGRQlhabu644w5uu+02brrpJurVq8err76Kv78/69atu+JzbDYboaGh+UtISEgJJhaX5+UFjz0GBw7Ayy9DQADe23Yy49NUvptmI3btbF5Y8oLVKUVEpBCl5mipvLw8pk+fzrlz54iJibniuIyMDCIjI4mIiLjqXh6A7Oxs0tLSCiwiV+XvDy++6Cw5o0aBhwe3xxt2fQhur77GtI2TrE4oIiJXYHm52b59O/7+/tjtdh555BFmz55Nw4YNLzs2KiqKzz77jLlz5/LVV1/hcDho3749x44du+Lrjxs3jqCgoPwlIiKiuFZFXFGVKvDOO7BtG3TujM8FeGUptO41nN1fvmN1OhERuQzLj5bKyckhISGB1NRUZs2axaeffsry5cuvWHB+LTc3lwYNGjB48GBeeeWVy47Jzs4mOzs7/3ZaWhoRERE6WkqunzE4pk0lZeRwKqY4/5s6f0cvfD/4GGrUsDiciIhrK9OHgnfr1o06derw8ccfX9P4u+++Gw8PD6ZNm3ZN43UouPxeGT+f4Jt7mjFkySk8DBhfX2wvvABPPeWcsyMiIkWuzBwKfjkOh6PAnpbC5OXlsX37dsLCwoo5lcgv/CuHc+t/N9HtyYqsqAG28+dh9GiIjobFi62OJyJS7llabkaPHs2KFSs4fPgw27dvZ/To0SxbtoyhQ4cCMGzYMEaPHp0/fuzYsSxYsICDBw+yZcsW7r33Xo4cOcKDDz5o1SpIOVUjqAbjRn1P9wc9ua8/ZFTwhfh46NYNBg2C48etjigiUm5ZWm5OnjzJsGHDiIqKomvXrmzcuJH58+fTvXt3ABISEkhMTMwff/bsWR566CEaNGjAbbfdRlpaGmvWrLmm+TkiRS0mIob/9P2Mr5pC9T+fZ8+QHs7z5cyYAfXrO3+KiEiJK3Vzboqb5txIUfv74r8zbtU4vNy92NBqIk1f/ggunqvpiSfgjTfA09PakCIiZVyZnnMjUtb8o8s/6Fe/Hzl5OfTY8TeOfPeV81pVAO+9B50762sqEZESpHIj8ju52dz4sv+XNA1pyslzJ7lz1gAyXn4O5syBwEBYvRpatIClS62OKiJSLqjciBQBfy9/vh38LSF+IWxL3sbQb4Zy/rbusHmz8yiqkyedk41ff915JXIRESk2KjciRaRGUA1mD5qNl7sX38Z/S9336/JxymJyV62AYcPA4YD/+z8YMABSU62OKyLislRuRIpQTEQMcwbNoWaFmiRmJPLID4/QaHJrZjxzG44J/3ae5G/OHGjVynlJBxERKXIqNyJFrPdNvdkzcg/v93qfKr5V2HdmH4P+ew9t3P7D+ulvOS/VsH8/tGsHX31ldVwREZejQ8FFilF6djrvrHuHN9a8QUZOBgD9K3dg0swcgpavdw4aMcJ5cU673cKkIiKlW5m+tlRxU7kRK5w6d4rXVr7Gvzf9m5y8HNwcMG1nAwb+d7dzQJMmMHw43H03hIdbG1ZEpBRSuSmEyo1Y6XDKYcYsG8MXW7/AYLh9nxvT53ril/G/66nZbNChg/MSDnfdBSEh1gYWESklVG4KoXIjpcGOkzt4bslzfBv/LVUy4L7dHvzlUFVq7jrxyyA3N7j1VmfRGTAAKle2LK+IiNVUbgqhciOlyeqE1Ty7+FlWJawCoFFmAO+m30LnDSdx37T5l4Hu7s7z5AwcCP37Q3CwRYlFRKyhclMIlRspbYwxzNs/j9GLR7Mt2Xl4eIhfCG/UfZTBuz3wmDkLYmN/eYKnJ9x7L/z73+DtbVFqEZGSpXJTCJUbKa0cxsH0HdN5YekLHDx7EIDawbV5pfMr3OPVAreZs5xXGt++3fmEbt2c58zx87MutIhICdGFM0XKIDebG0OaDGH3yN18eNuHhPiFcPDsQYZ+M5TmSwfx46AWmK1bYcECZ6FZtAh69tTZjkVEfkPlRqSU8XL34tHWj3Lg8QO82uVVAu2BbEveRp+pfeg4uSOronxg4UIICnJelLNrV/j5Z6tji4iUGio3IqWUn5cff+/wdw4+fpBn2j+Dt4c3qxJW0WFSB+46+iZnf/zGeQTV5s3Oo6oSE62OLCJSKqjciJRylXwr8Xr319n3l3081OIh3G3ufLP7GxqtvZc1U8Y7T/q3cyd07AhHjlgdV0TEcio3ImVE9cDqTLxjIpsf3kyDyg1IzEjk5rUPMn5cH0ytms7rVXXoAHv3Wh1VRMRSKjciZUzT0KZsengTf275ZwBGH/qE2x8JJKduLTh61LkH5+IRVSIi5ZDKjUgZ5Ovpy0e3f8R/B/6XYO9gfszcRtTdyZypFwHJyc45OJs2WR1TRMQSKjciZdiABgPYNmIbnSI7cdh+njr9j7Lvpkpw5gx06QIrV1odUUSkxKnciJRx1QOrs3jYYl7t8irpvu60+MNp1tb1hvR053lwFiywOqKISIlSuRFxAe5u7vy9w99Z9adVVAmpRZdBWfx4E5CZibnjDpg2DcrXychFpBxTuRFxIe2qtyP2z7EMaDGEfoNgZkOw5eTAkCHk3d4HDh2yOqKISLHzsDqAiBStIO8gpgyYQq86vXjIPoL4Bef422rw+nEeWVF1mNU/ir0P3EmD6s1oVLURUZWisHvYrY4tIlJkdOFMERd24MwBRs0fxanNK3n1m1S6/m/HTXwleLQPLKkN7jZ36lasS6OqjWhUpRGNqzamd93eBNgDrA0vIvIruip4IVRupDwyxpCckcTP//mAWv/4AL/TaQDMaurJX7rlkvSbHlO3Yl1+GPID9SrVsyCtiMilVG4KoXIj5V5qKrzwAnz4ITgcOAL82TvqPub3qMP2M7uZt38eJ9JPEOwdzDeDvuHWmrdanVhEROWmMCo3Iv+zZQuMGAEbNjhvN2sGEyaQ3LgWfaf3Zf3x9Xi6eTLxjok80OwBK5OKiFzX57eOlhIpr1q0gLVr4eOPITgY4uIgJoaQp15gafcpDGw0kFxHLn+c+0eeW/wcDuOwOrGIyDVRuREpz9zc4OGHIT4e/vhH532ffIJPrZuY/uFJ5qT0ptI5eG3VawyaNYjM3Exr84qIXAN9LSUiv1i1CkaPdv78H4e7GwtqGaY2Nhzr3IKpf/qBUP9QC0OKSHmkOTeFULkRuQZHjsCMGc4zG8fG5t+d5Q7LGvrQ8C9jqTH0UfD1tTCkiJQnKjeFULkRuU7x8fD11+R89Tle+w7m333B1xuPfgPgnnugd2/w0DlBRaT4aEKxiBSdqCh48UW84veTun4FX/WJ4GAF8DifBVOnwp13Qt268N57kJFhdVoREZUbEblGNhtBbTowcO5+Xpl0P20fhHfaQXqQj/NrrFGjoEYNeP55SE62Oq2IlGMqNyJyXbzcvfis7yT63z+Op3pB1ccyeX5gZc5GVIGzZ+HVVyEy8pejsERESpjKjYhcN5vNxrO3PMvMu2fiG1iRVxv+TOU/nuLee3053CAMsrPhk0+gQQPo3x/WrLE6soiUIyo3InLD/tDwDySMSuDft/2bulXqMaXueWoNTKTTn9zY2CocjIE5c+Dmm+GWW2DuXHDoZIAiUrx0tJSIFAmHcTBv3zzeWfcOiw8tBqD+Kfjn1ir0WX8W99wLzoH168Nbb8Ftt1mYVkTKGh0KXgiVG5Hity15G++ue5cp26eQk5dDaDo8HxvA8A25eGdkOQcNHgzvvgtVq1qaVUTKhjJzKPiECROIjo4mMDCQwMBAYmJimDdvXqHPmTlzJvXr18fb25smTZrw448/llBaEblW0SHRfNb3MxJGJfBSp5fIC6nCYx3TqfpYFu/d4oHDBkybhqNBffjiC+fXVyIiRcTSclO9enXGjx/P5s2b2bRpE126dKFv377s3LnzsuPXrFnD4MGDGT58OLGxsfTr149+/fqxY8eOEk4uItcixD+EMbeOIeHJBP5z53+oWaMJo7pdoM2DEBcCbmfOwv33czymMad3brI6roi4iFL3tVTFihV54403GD58+CWPDRo0iHPnzvH999/n39euXTuaNWvGRx99dE2vr6+lRKxjjGHjiY3M3j2bb3f+lzu+28eYZeCdB+c8YfJdtXE89hj9Gv+BiKAIq+OKSClSZr6W+rW8vDymT5/OuXPniImJueyYtWvX0q1btwL39ezZk7Vr117xdbOzs0lLSyuwiIg1bDYbbaq1YVy3cex4PJ5h03byny9GsameP365MHL6QWLufoo7/q8GrT9pzbiV44j/WefKEZHrY3m52b59O/7+/tjtdh555BFmz55Nw4YNLzs2KSmJkJCQAveFhISQlJR0xdcfN24cQUFB+UtEhP5vUKQ0sNlsNKzSkJFD3qHVnjR+fn88WQE+tEqETRPhrs83MXb+36n/YX1aTWzFe+ve4+S5k1bHFpEywPJyExUVRVxcHOvXr2fEiBHcf//97Nq1q8hef/To0aSmpuYvR48eLbLXFpEiYrNR+S//h/feg3D33XgYeHY17P/Ul26H3dicuJlR80cR/lY4fab2YfqO6WTmZlqdWkRKKcvLjZeXF3Xr1qVly5aMGzeOpk2b8t577112bGhoKMm/uWZNcnIyoaGhV3x9u92efzTWxUVESqnQUJgxw3niv/BwqiWfZ+FkB4fnN2B4ZgPyTB4/7vuRwf8dTMibIfxp7p9YemgpDqMTA4rILywvN7/lcDjIzs6+7GMxMTEsXry4wH0LFy684hwdESmj+vaFXbvg0UfB3Z3Itbv59J+7yVgSw6eB91GzQk3Sc9KZFDeJLl90oea7NRm9aDS7ThXdXl8RKbssPVpq9OjR9O7dmxo1apCens7UqVP55z//yfz58+nevTvDhg2jWrVqjBs3DnAeCt6pUyfGjx9Pnz59mD59Oq+99hpbtmyhcePG1/SeOlpKpIw5cADGjYPPP4cLzrMcm44d2fHn/rzvt4OZu2eRmp2aP7x1eGv+GvNX7mp4Fx5uHlalFpEiVmaOljp58iTDhg0jKiqKrl27snHjxvxiA5CQkEBiYmL++Pbt2zN16lQmTpxI06ZNmTVrFnPmzLnmYiMiZVCdOvDpp7B/PzzyCHh5YVuxgiZDn+STf+7iZJMvmHHX19xR7w483DzYeGIj9/z3HqI+iGLCxgmamyNSDpW689wUN+25ESnjjh2D1193XnU863+XcmjVCl58kVOd2/LvTRP414Z/cTrzNABV/aryeJvHebT1owT7BDvHGwPnz0NKCgQHg6+vNesiItdM15YqhMqNiItISoI334QJE5xFBaBZM3jgAXLOpbMtfjl79q/DIzWDCllQKduNSBNEpWw33FPTIDfX+Rw/P3jwQXjySYiMtGx1RKRwKjeFULkRcTEnT8Lbb8OHH0JGxvU912b75bpW7u4waBA884yzJIlIqaJyUwiVGxEXdfo0fPABxMZCUJDz66YKFSA4GBMUxJbsw3xxZC5LUuJI8YYUb+jS6Hb+4biVxp/Pw/brIzG7dXOWnO7dnQVIRCynclMIlRuR8m39sfW8vuZ1Zu+ejcH5z1+d4Dr8zacbQ+Yn4j/nB8jLcw5u2tRZcgYOBE9PC1OLiMpNIVRuRAQg/ud43lr7FtN2TCMj55evswb5teWFLf40nLsW28W5PDVqwKhRzrk5AQHWBBYp51RuCqFyIyK/di7nHLP3zObzrZ+z+ODi/L051XK8eedQPe5ckID9dIpzcIUKMGAAREc7lyZNoHJly7KLlCcqN4VQuRGRKzmaepSvtn3F51s/J/6082rk3rnw+N4KPLPGRuXjZy99UliYs+Q0afJL4WnQALy9Szi9iGtTuSmEyo2IXI0xhg3HN/D51s+ZvmM6Z7PO4uaAnvvhzjOViUkJoM7Rc/gfv8JVyt3doV49Z9np1g3+8AfnXh8RuWEqN4VQuRGR65F9IZvv9n7H51s/Z96+eeSZvPzH/LOhZ1Y17siKpM0ZH2oey8B7115sZ3+zh8duhzvvhPvug549wcurhNdCpOxTuSmEyo2I3Kifz//MiiMr8petyVsvuSJ5Nf9w7gxoxW1ZEbQ5YaPK90ux7dz5y4BKleCee5xFp00bHWouco1UbgqhciMiRSU1K5U1R9c4y07CCjYe30iuI7fAmIiA6jxh78TguAuEfbcMW3LyLw/Wqwf33utcatUq2fAiZYzKTSFUbkSkuJzPPc+G4xvy9+ysObqGzAu/XLgz0q8az2a1YsDmTKosWPXLoeYAt9zi3Jtz993OExCKSAEqN4VQuRGRkpKZm8n8A/OZuWsm38V/R3pOev5j9TzDeOFMY/qsP0uF1ZuxXfyn2NMTbrsNhg6F228HHx+L0ouULio3hVC5ERErZF3IYv7++czaPYtv478lLTst/7HmeVV58Xhduq1Jxn/PgV+eFBjoPK/O0KHQubPzKCyRckrlphAqNyJitewL2Sw4sICZu2YyN35ugaLT7qw/fz0USo8NpwlM+tVRV2FhzonIQ4dCixaaiCzljspNIVRuRKQ0yb6QzaKDi/KLTkpWCgA2B9x8FB7c7cOAHXkEZOT88qSoKBgyBAYPhrp1VXSkXFC5KYTKjYiUVnmOPLYkbmHJoSUsPrSYVQmryLyQiecF6LUfhm6HvntteOf+6p9tf3+oXRvq1Cn4s3ZtiIzUOXXEZajcFELlRkTKiuwL2aw7to7Fhxaz5NAS1h9fj8/5C/TfA0O2Q7eD4F7Iv+DGzQ0iIrBdLDxRUXDHHc6fImWMyk0hVG5EpKxKz05nZcLK/D07u4/GUTMFap91LnUu/jzj/Ol74fKvk924Pl6D78M2aJBzT49IGaByUwiVGxFxFedyznEk9QiHUw5zJMX583Dq/34/ewhb8slfis8ZaHcMuh4Cz1+dVDkxKpxz/W8n7E9P4HdTQ+tWRuQqVG4KoXIjIuVFZm4mCakJzvKTeoTdp3azK34ltZbGcdf2PLocKvi11raavsR3bYq5+w80bdmHepXqYdNkZSklVG4KoXIjIuVd9oVsYpNiidu6AI85c2m4dCftDmTj9qsxqyLgx6a+pLdtRkhMd9rVuoU21doQaNe/m2INlZtCqNyIiFwqce8Wkj//kKC586m183iBxzI8YX11WBMBx6NrYr/5VppFdSQmIoZ6lerhZnO7wquKFB2Vm0Ko3IiIXMWxY1z4ehoZ38/GZ1Mc9ozMAg87gB1VYXUN2Frbl5x2ranVvAvtImJoXa01FbwrWBJbXJvKTSFUbkREroPDATt3wurVnF++GMfqlfgfTb5kWKI/rKsOsaFwql41vFu3o26TW2lTvS1NQ5vi5a7z7cjvo3JTCJUbEZHfKTER1q4lb+UKMpcvxmf7btwv5F0y7GcfiA2D7WFupDSohXerGGq26UHrGu2oW7GuJivLdVG5KYTKjYhIEcvMhI0bYdMmsjatI2fzBvz2J+DuuPTj5bwHbAuB3dW9OHVTNc7UCSfjppr4h1Snsm9lqvhWcf70c/6s7FuZAK8AFSFRuSmMyo2ISAnIyoIdOzBbtpC6fgW5mzcQuOcQ9uzLn1kwIdA5j2d7yP9+VoXdVSDHA7zcvajsW5lmoc24o94d3F7vdqoHVr/8+xoD589DejpUqADe3sW3jlKiVG4KoXIjImKRvDzYv58LWzZxavUCbNu247/3MP7JZy87/IIb7KvoLDo7qkKWBwRlQ1AW1HILpq57FcId/vhn5mFLTYXUVEhLc77PRRUrQng4VKv2y89f/x4eDlWrgrt7Cf0R5Eap3BRC5UZEpJRJSYEdO5zL9u2/LCkpJfP+7u7OknPbbXDffdC+/WWvtJ6Zm0lsUizrjq3jWNox+tzUh861OutQ+BKiclMIlRsRkTLAGDhxwllyduxwHrHlcEBQEOd8PNiVe5xN5/axNm0XyR7ZpNkh1Q45/t60qN+Z7k360cKnNtXSoUpKLu6JSc7XO3684M+kJOfr/lqdOph77+XwHR1Y5XGCdcfWsf74erYmb+WCo+DXanWC6/BQi4d4oNkDhPiHXPv6ZWTA4cNw6JDz529/9/aGhx+GESMg5Dpe14Wp3BRC5UZExHVkXchi2eFlfBf/Hd/t/Y6jaUcvGeNmcyPEL4TwgHCqBVajWoBzCQ8Ip7pvKDWyval0MImsLz+jyrwV2DNz8p+7sgZ80RRmNoRUHwjxC6Ft9bZU9KnIf3f9l/ScdAA83DzoV78fD7d4mK61uzr35qSmOkvZrl0QH1+wxJw+fW0raLfD0KHw5JPQuHER/MXKLpWbQqjciIi4JmMM25K38d3e71hwYAGHUg6RmJ5Inrn0MPUr8c2Bfntg2FbodvCXa2/leXmSdVt3fP/0CLZevcDTk3M55/h659dMXflvzm/dTKNT0PAUtDrrTdMzXgSeSiv8zSpWhJo1nUutWr/8XrOmsxC99RZs2PDL+B49nCWnZ8/Lfm3m6lRuCqFyIyJSfuQ58jh1/hQn0k9wPO04x9OPX/p7+nHOZJ6hXqV6tK3W1rlUb0v0hUp4fT0LPv/cuQfmoipVoHdv5/l+du50fr11BacremMaNqRS85sxtWuRHlaJn0MCSKzoRZJHJqfOneLU+VO//Pzf717uXnSp2ZmBqRE0n74U9znf/vL1WcOGzpIzdCj4+BTzX7D0ULkphMqNiIj8Vp4jD3e3KxwxZQzExcGXX8KUKXDy5KVjqlWDRo3IbVCPjYHnmHxhEzMc20n9X/fw9/LnfO55HMZx6XOvwsfDh4G+rXl8PTT9fhPu5847H6hSxTkn59FHy8W8HJWbQqjciIjIDbtwARYsgLVrITISGjVy7kkJCrpk6I6TO/hk8yd8se0LUrJS8u8P9g6mil8VqvhW+eXnr36v6leVU+dPseDAAhYcWEBiRmL+cwOz4KldgYxcm0flU+ecd3p5QefOULmy89w+wcFQoQKOCkFk+tlJ9bGR4mPjjN3Bz155nPTIom7lenSK7HTlQnc1eXnOeUPJyZdf2rWDRx65sde+ApWbQqjciIhIScrMzeRwymGCfYKp5FMJT3fPa36uMYadp3bmF53lR5aTdSEL9zwYsBueWgvtjl/9dX7NAZz3hGwvN9x8/fANrIjdL8j5FddvF29v55KaWrC8nDp16VFmv5JzV3+8Zn1zfcGuQuWmECo3IiJSVmVdyGLlkZXOsnNwAduSt9HqODRJhgpZEJz1v5+Zv9wOzrZRMctGUKbBJ7eIP/IrVcJRtSqpFewc87nAXo8U4kwSNTv1Zfhzs4r0rVRuCqFyIyIiriIxPZHFhxbz8/mfqehTkWDvYOdPn2CCvYMJ9gnG2+NXl6DIyoKUFLJTz7AyfgHzd8xl04GVeObk4XMBfPNstKjQkI5VWtEsKAr7BeO8dlhAgHNeT0gIaUE+rL9wmMUZ21h+Yg2bTmy65Pw/Per0YP6984t0XVVuCqFyIyIi8oszmWeYsXMGX277kjVH1+Tf7+vpy4AGAxjceDAZORmsPLKSFQkr2J68HUPB6lA9sDodIzvSsUZHOkR2oEHlBkV+sVOVm0Ko3IiIiFzegTMHmLJ9Cl9u+5L9Z/ZfcVy9SvXoWKMjHSOdZSYyKLLYr9yuclMIlRsREZHCGWNYf3w9X279km/3fktl38r5e2U61OhwfZeaKCJlptyMGzeOb775hj179uDj40P79u355z//SVRU1BWfM3nyZP74xz8WuM9ut5OVlXVN76lyIyIiUvZcz+e3pZcyXb58OSNHjmTdunUsXLiQ3NxcevTowblz5wp9XmBgIImJifnLkSNHSiixiIiIlHYeVr75Tz/9VOD25MmTqVq1Kps3b6Zjx45XfJ7NZiM0NPSa3iM7O5vs7Oz822lpV7nWh4iIiJRplu65+a3U1FQAKlasWOi4jIwMIiMjiYiIoG/fvuz89TU/fmPcuHEEBQXlLxEREUWaWUREREqXUjOh2OFwcOedd5KSksKqVauuOG7t2rXs27eP6OhoUlNTefPNN1mxYgU7d+6kevXql4y/3J6biIgIzbkREREpQ8rMhOJfGzFiBPPmzWPVqlWXLSlXkpubS4MGDRg8eDCvvPLKVcdrQrGIiEjZcz2f35bOubnoscce4/vvv2fFihXXVWwAPD09ad68Ofv3X/l4fBERESk/LJ1zY4zhscceY/bs2SxZsoRatWpd92vk5eWxfft2wsLCiiGhiIiIlDWW7rkZOXIkU6dOZe7cuQQEBJCUlARAUFAQPj4+AAwbNoxq1aoxbtw4AMaOHUu7du2oW7cuKSkpvPHGGxw5coQHH3zQsvUQERGR0sPScjNhwgQAbr311gL3T5o0iQceeACAhIQE3Nx+2cF09uxZHnroIZKSkggODqZly5asWbOGhg0bllRsERERKcVKzYTikqIJxSIiImVPmTlDsYiIiEhRU7kRERERl6JyIyIiIi5F5UZERERcSqk4iV9Jujh/WhfQFBERKTsufm5fy3FQ5a7cpKenA+gCmiIiImVQeno6QUFBhY4pd4eCOxwOTpw4QUBAADabrUhf++JFOY8ePeryh5lrXV1XeVpfravrKk/rW17W1RhDeno64eHhBc5/dznlbs+Nm5vbdV+/6noFBga69H9gv6Z1dV3laX21rq6rPK1veVjXq+2xuUgTikVERMSlqNyIiIiIS1G5KUJ2u52XXnoJu91udZRip3V1XeVpfbWurqs8rW95WtdrVe4mFIuIiIhr054bERERcSkqNyIiIuJSVG5ERETEpajciIiIiEtRuSkiH374ITVr1sTb25u2bduyYcMGqyMVizFjxmCz2Qos9evXtzpWkVixYgV33HEH4eHh2Gw25syZU+BxYwwvvvgiYWFh+Pj40K1bN/bt22dN2N/pauv6wAMPXLKde/XqZU3Y32ncuHG0bt2agIAAqlatSr9+/YiPjy8wJisri5EjR1KpUiX8/f256667SE5Otijx73Mt63vrrbdesn0feeQRixLfuAkTJhAdHZ1/8rqYmBjmzZuX/7grbderraurbNOionJTBL7++mueeuopXnrpJbZs2ULTpk3p2bMnJ0+etDpasWjUqBGJiYn5y6pVq6yOVCTOnTtH06ZN+fDDDy/7+Ouvv87777/PRx99xPr16/Hz86Nnz55kZWWVcNLf72rrCtCrV68C23natGklmLDoLF++nJEjR7Ju3ToWLlxIbm4uPXr04Ny5c/ljnnzySb777jtmzpzJ8uXLOXHiBAMGDLAw9Y27lvUFeOihhwps39dff92ixDeuevXqjB8/ns2bN7Np0ya6dOlC37592blzJ+Ba2/Vq6wqusU2LjJHfrU2bNmbkyJH5t/Py8kx4eLgZN26chamKx0svvWSaNm1qdYxiB5jZs2fn33Y4HCY0NNS88cYb+felpKQYu91upk2bZkHCovPbdTXGmPvvv9/07dvXkjzF7eTJkwYwy5cvN8Y4t6Onp6eZOXNm/pjdu3cbwKxdu9aqmEXmt+trjDGdOnUyTzzxhHWhilFwcLD59NNPXX67GvPLuhrj2tv0RmjPze+Uk5PD5s2b6datW/59bm5udOvWjbVr11qYrPjs27eP8PBwateuzdChQ0lISLA6UrE7dOgQSUlJBbZzUFAQbdu2ddntvGzZMqpWrUpUVBQjRozg9OnTVkcqEqmpqQBUrFgRgM2bN5Obm1tg29avX58aNWq4xLb97fpeNGXKFCpXrkzjxo0ZPXo058+ftyJekcnLy2P69OmcO3eOmJgYl96uv13Xi1xtm/4e5e7CmUXt559/Ji8vj5CQkAL3h4SEsGfPHotSFZ+2bdsyefJkoqKiSExM5OWXX6ZDhw7s2LGDgIAAq+MVm6SkJIDLbueLj7mSXr16MWDAAGrVqsWBAwf4+9//Tu/evVm7di3u7u5Wx7thDoeDUaNGcfPNN9O4cWPAuW29vLyoUKFCgbGusG0vt74AQ4YMITIykvDwcLZt28b//d//ER8fzzfffGNh2huzfft2YmJiyMrKwt/fn9mzZ9OwYUPi4uJcbrteaV3BtbZpUVC5kevSu3fv/N+jo6Np27YtkZGRzJgxg+HDh1uYTIrSPffck/97kyZNiI6Opk6dOixbtoyuXbtamOz3GTlyJDt27HCZeWJXc6X1ffjhh/N/b9KkCWFhYXTt2pUDBw5Qp06dko75u0RFRREXF0dqaiqzZs3i/vvvZ/ny5VbHKhZXWteGDRu61DYtCvpa6neqXLky7u7ul8zAT05OJjQ01KJUJadChQrUq1eP/fv3Wx2lWF3cluV1O9euXZvKlSuX6e382GOP8f3337N06VKqV6+ef39oaCg5OTmkpKQUGF/Wt+2V1vdy2rZtC1Amt6+Xlxd169alZcuWjBs3jqZNm/Lee++55Ha90rpeTlnepkVB5eZ38vLyomXLlixevDj/PofDweLFiwt8F+qqMjIyOHDgAGFhYVZHKVa1atUiNDS0wHZOS0tj/fr15WI7Hzt2jNOnT5fJ7WyM4bHHHmP27NksWbKEWrVqFXi8ZcuWeHp6Fti28fHxJCQklMlte7X1vZy4uDiAMrl9f8vhcJCdne1y2/VyLq7r5bjSNr0hVs9odgXTp083drvdTJ482ezatcs8/PDDpkKFCiYpKcnqaEXur3/9q1m2bJk5dOiQWb16tenWrZupXLmyOXnypNXRfrf09HQTGxtrYmNjDWDefvttExsba44cOWKMMWb8+PGmQoUKZu7cuWbbtm2mb9++platWiYzM9Pi5NevsHVNT083Tz/9tFm7dq05dOiQWbRokWnRooW56aabTFZWltXRr9uIESNMUFCQWbZsmUlMTMxfzp8/nz/mkUceMTVq1DBLliwxmzZtMjExMSYmJsbC1Dfuauu7f/9+M3bsWLNp0yZz6NAhM3fuXFO7dm3TsWNHi5Nfv2effdYsX77cHDp0yGzbts08++yzxmazmQULFhhjXGu7FraurrRNi4rKTRH517/+ZWrUqGG8vLxMmzZtzLp166yOVCwGDRpkwsLCjJeXl6lWrZoZNGiQ2b9/v9WxisTSpUsNcMly//33G2Och4O/8MILJiQkxNjtdtO1a1cTHx9vbegbVNi6nj9/3vTo0cNUqVLFeHp6msjISPPQQw+V2bJ+ufUEzKRJk/LHZGZmmkcffdQEBwcbX19f079/f5OYmGhd6N/hauubkJBgOnbsaCpWrGjsdrupW7eueeaZZ0xqaqq1wW/An/70JxMZGWm8vLxMlSpVTNeuXfOLjTGutV0LW1dX2qZFxWaMMSW3n0hERESkeGnOjYiIiLgUlRsRERFxKSo3IiIi4lJUbkRERMSlqNyIiIiIS1G5EREREZeiciMiIiIuReVGREREXIrKjYhIObNs2TJsNtslF5UUcRUqNyIWOHXqFCNGjKBGjRrY7XZCQ0Pp2bMnq1evzh9js9mYM2eOdSGvw8UPy8stSUlJVse7RGJiIkOGDKFevXq4ubkxatSoy46bOXMm9evXx9vbmyZNmvDjjz8WeNwYw4svvkhYWBg+Pj5069aNffv2lcAaiEhhVG5ELHDXXXcRGxvL559/zt69e/n222+59dZbOX36tNXRfpf4+HgSExMLLFWrVi2298vJybmh52VnZ1OlShWef/55mjZtetkxa9asYfDgwQwfPpzY2Fj69etHv3792LFjR/6Y119/nffff5+PPvqI9evX4+fnR8+ePcnKyrqhXCJSRCy+tpVIuXP27FkDmGXLll1xTGRkZIGLHkZGRuY/NmfOHNO8eXNjt9tNrVq1zJgxY0xubm7+44D597//bXr16mW8vb1NrVq1zMyZM/Mfz87ONiNHjjShoaHGbrebGjVqmNdee+13rdPFC3GePXv2so/Pnz/f2O32Sx5//PHHTefOnfNvr1y50txyyy3G29vbVK9e3fzlL38xGRkZBf4uY8eONffdd58JCAgw999/v+ncubMZOXJkgdc9efKk8fT0NIsWLbpq9k6dOpknnnjikvsHDhxo+vTpU+C+tm3bmj//+c/GGOeFVENDQ80bb7yR/3hKSoqx2+1m2rRpV3y/vLw889prr5maNWsab29vEx0dXWD7XPxbfv/996ZJkybGbrebtm3bmu3btxd4nVmzZpmGDRsaLy8vExkZad58880Cj2dlZZm//e1vpnr16sbLy8vUqVPHfPrppwXeY9GiRaZly5bGx8fHxMTEmD179uQ/Py4uztx6663G39/fBAQEmBYtWpiNGzde5a8pUjqo3IiUsNzcXOPv729GjRplsrKyLjvm5MmT+VdyTkxMNCdPnjTGGLNixQoTGBhoJk+ebA4cOGAWLFhgatasacaMGZP/XMBUqlTJfPLJJyY+Pt48//zzxt3d3ezatcsYY8wbb7xhIiIizIoVK8zhw4fNypUrzdSpU3/XOl2t3Fy4cMGEhITkf7he7r79+/cbPz8/884775i9e/ea1atXm+bNm5sHHngg/zmRkZEmMDDQvPnmm2b//v1m//79ZsqUKSY4OLjA3/Ltt982NWvWNA6H46rZr1RuIiIizDvvvFPgvhdffNFER0cbY4w5cOCAAUxsbGyBMR07djSPP/74Fd/vH//4h6lfv7756aefzIEDB8ykSZOM3W7PL7sX/5YNGjQwCxYsMNu2bTO33367qVmzpsnJyTHGGLNp0ybj5uZmxo4da+Lj482kSZOMj49PgSudDxw40ERERJhvvvnGHDhwwCxatMhMnz69wHu0bdvWLFu2zOzcudN06NDBtG/fPv/5jRo1Mvfee6/ZvXu32bt3r5kxY4aJi4u76t9TpDRQuRGxwKxZs0xwcLDx9vY27du3N6NHjzZbt24tMAYws2fPLnBf165dL9nL8uWXX5qwsLACz3vkkUcKjGnbtq0ZMWKEMcaYv/zlL6ZLly7X9MF/rS5+WPr5+RVYGjZsmD/miSeeMF26dMm//du9OcOHDzcPP/xwgddduXKlcXNzM5mZmcYYZ7np169fgTGZmZkmODjYfP311/n3RUdHFyh8hblSufH09Lyk9H344YematWqxhhjVq9ebQBz4sSJAmPuvvtuM3DgwMu+V1ZWlvH19TVr1qwpcP/w4cPN4MGDjTG//C0vFhFjjDl9+rTx8fHJX8chQ4aY7t27F3iNZ555Jv/vHR8fbwCzcOHCy+b49Z6bi3744QcD5P+tAwICzOTJky/7fJHSTnNuRCxw1113ceLECb799lt69erFsmXLaNGiBZMnTy70eVu3bmXs2LH4+/vnLw899BCJiYmcP38+f1xMTEyB58XExLB7924AHnjgAeLi4oiKiuLxxx9nwYIFV3y/lStXFnivKVOmFJpv5cqVxMXF5S+/noA7dOhQli1bxokTJwCYMmUKffr0oUKFCvnrNnny5ALv17NnTxwOB4cOHcp/nVatWhV4T29vb+677z4+++wzALZs2cKOHTt44IEHCs1qhf3793P+/Hm6d+9eYD2/+OILDhw4UGDsr7dhxYoViYqKyt+Gu3fv5uabby4w/uabb2bfvn3k5eURFxeHu7s7nTp1KjRPdHR0/u9hYWEAnDx5EoCnnnqKBx98kG7dujF+/PhL8omUZh5WBxApr7y9venevTvdu3fnhRde4MEHH+Sll14q9EM5IyODl19+mQEDBlz29a5FixYtOHToEPPmzWPRokUMHDiQbt26MWvWrEvGtmrViri4uPzbISEhhb52rVq18svKb7Vu3Zo6deowffp0RowYwezZswuUuYyMDP785z/z+OOPX/LcGjVq5P/u5+d3yeMPPvggzZo149ixY0yaNIkuXboQGRlZaNarCQ0NJTk5ucB9ycnJhIaG5j9+8b6LxeDi7WbNml32NTMyMgD44YcfqFatWoHH7Hb778r7az4+Ptc0ztPTM/93m80GgMPhAGDMmDEMGTKEH374gXnz5vHSSy8xffp0+vfvX2Q5RYqLyo1IKdGwYcMCh357enqSl5dXYEyLFi2Ij4+nbt26hb7WunXrGDZsWIHbzZs3z78dGBjIoEGDGDRoEH/4wx/o1asXZ86coWLFigVex8fH56rvdT2GDh3KlClTqF69Om5ubvTp0yf/sRYtWrBr164ber8mTZrQqlUrPvnkE6ZOncoHH3zwu7PGxMSwePHiAoeJL1y4MH+PSq1atQgNDWXx4sX5ZSYtLY3169czYsSIy75mw4YNsdvtJCQkXHWvyrp16/JL3dmzZ9m7dy8NGjQAoEGDBgVOGwCwevVq6tWrh7u7O02aNMHhcLB8+XK6det2I6sPQL169ahXrx5PPvkkgwcPZtKkSSo3UjZY/b2YSHnz888/m86dO5svv/zSbN261Rw8eNDMmDHDhISEmD/96U/542666SYzYsQIk5iYaM6cOWOMMeann34yHh4eZsyYMWbHjh1m165dZtq0aea5557Lfx5gKleubP7zn/+Y+Ph48+KLLxo3Nzezc+dOY4wxb731lpk6darZvXu3iY+PN8OHDzehoaEmLy/vhtfp4hyO+Ph4k5iYWGC5OAnWGGP27dtnABMdHW2GDx9e4DW2bt1qfHx8zMiRI01sbKzZu3evmTNnToEjoSIjIy+Z5HvRxIkTjZeXlwkODs6fN1KY2NhYExsba1q2bGmGDBliYmNj8/9Gxjjn1Hh4eJg333zT7N6927z00kvG09OzwFFL48ePNxUqVDBz584127ZtM3379jW1atUq9P2fe+45U6lSJTN58mSzf/9+s3nzZvP+++/nz2+5+Lds1KiRWbRokdm+fbu58847TY0aNUx2drYxxpjNmzcXmFA8efLkSyYUP/DAAyYiIsLMnj3bHDx40CxdujR/zs7lJoDHxsYawBw6dMicP3/ejBw50ixdutQcPnzYrFq1ytSpU8f87W9/u+rfVaQ0ULkRKWFZWVnm2WefNS1atDBBQUHG19fXREVFmeeff96cP38+f9y3335r6tatazw8PAocCv7TTz+Z9u3bGx8fHxMYGGjatGljJk6cmP84YD788EPTvXt3Y7fbTc2aNQtMtp04caJp1qyZ8fPzM4GBgaZr165my5Ytv2udLn5YXm5Zu3ZtgbFt2rQxgFmyZMklr7NhwwbTvXt34+/vb/z8/Ex0dLR59dVX8x8vrNykp6cbX19f8+ijj15T5stl/fXf2RhjZsyYYerVq2e8vLxMo0aNzA8//FDgcYfDYV544QUTEhJi7Ha76dq1q4mPjy/0fR0Oh3n33XdNVFSU8fT0NFWqVDE9e/Y0y5cvN8b88rf87rvvTKNGjYyXl5dp06bNJRPOLx4K7unpaWrUqFHgkHRjnBOtn3zySRMWFma8vLxM3bp1zWeffVbgPa5UbrKzs80999xjIiIijJeXlwkPDzePPfbYNZVGkdLAZowxJbmnSESKl81mY/bs2fTr18/qKCXq8OHD1KlTh40bN9KiRQur49ywZcuW0blzZ86ePXvF+UsiUjjNuRGRMi03N5fTp0/z/PPP065duzJdbESkaOhQcBEp01avXk1YWBgbN27ko48+sjqOiJQC+lpKREREXIr23IiIiIhLUbkRERERl6JyIyIiIi5F5UZERERcisqNiIiIuBSVGxEREXEpKjciIiLiUlRuRERExKX8P+Di2BUgxLOXAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "plt.plot(train_loss_list_converted, 'g', validation_loss_list_converted, 'r')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF0bV7e9du1A"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:45:44.322237Z",
          "iopub.status.busy": "2024-06-20T15:45:44.321316Z",
          "iopub.status.idle": "2024-06-20T15:45:46.887084Z",
          "shell.execute_reply": "2024-06-20T15:45:46.886126Z",
          "shell.execute_reply.started": "2024-06-20T15:45:44.322203Z"
        },
        "id": "E4bmoHgYdu1A",
        "outputId": "73643afe-6b23-4011-802f-132a12eeef21",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 33.51M\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Load the model\n",
        "nanoGPT = GPT.GPT(config)\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "nanoGPT.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-20T15:44:36.64937Z",
          "iopub.status.busy": "2024-06-20T15:44:36.64896Z",
          "iopub.status.idle": "2024-06-20T15:45:14.425576Z",
          "shell.execute_reply": "2024-06-20T15:45:14.424712Z",
          "shell.execute_reply.started": "2024-06-20T15:44:36.649341Z"
        },
        "id": "QBN6P6v0du1A",
        "outputId": "baf018c3-5440-4a68-9022-daf530c0e496",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There was a young child! She was wearing an strange phone! She had never seen all about the flowers before. \n",
            "\n",
            "John was surprised! \"What does I see today?\" Sally asked. Suddenly, she saw it was a beautiful star. It was bigger and fake. It had a vanish! \n",
            "\n",
            "John and Jane went inside and shouted. Little Jane's eyes looked like spotted it. Sammy jumped up and smiled. \n",
            "\n",
            "Jackmy's mum had been joining that the bully was much lonely! It gave Rover its happiness. Lisa was so relieved. She then asked Jane if he had come and go home now.\n",
            "\n",
            "Jack decided to come over and explore the project. He followed deeper and he finished, jumping. Peter quickly rain every branch. \n",
            "\n",
            "He filling the grass to the forest, quiet breeze from behind Little Jane was close to the ground. Jane smiled back where he was on. Then, a young bitggie: \" halfway... difficulties\" the happiness means the dancing\n"
          ]
        }
      ],
      "source": [
        "sentence = \"There was a\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = nanoGPT.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
